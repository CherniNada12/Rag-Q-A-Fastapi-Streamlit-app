"""
G√©n√©rateur de r√©ponses sp√©cialis√© pour l'assistant p√©dagogique
"""
from typing import List, Dict
import logging
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from .config import config
from .learning_config import learning_config

logger = logging.getLogger(__name__)

class LearningResponseGenerator:
    """G√©n√©rateur de r√©ponses p√©dagogiques adaptatif"""
    
    def __init__(self, model_name: str = None, use_openai: bool = False):
        self.model_name = model_name or config.LLM_MODEL
        self.use_openai = use_openai
        
        if use_openai and config.OPENAI_API_KEY:
            self._init_openai()
        else:
            self._init_local_model()
    
    def _init_openai(self):
        """Initialise OpenAI"""
        try:
            import openai
            openai.api_key = config.OPENAI_API_KEY
            self.client = openai.OpenAI()
            logger.info("‚úÖ Client OpenAI initialis√©")
        except Exception as e:
            logger.error(f"Erreur OpenAI : {e}")
            self._init_local_model()
    
    def _init_local_model(self):
        """Initialise un mod√®le local"""
        logger.info(f"Chargement du mod√®le : {self.model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_new_tokens=300,
                temperature=0.7
            )
            logger.info("‚úÖ Mod√®le local charg√©")
        except Exception as e:
            logger.error(f"Erreur chargement : {e}")
            raise
    
    def generate_pedagogical_answer(
        self,
        question: str,
        context_chunks: List[Dict],
        learning_level: str = 'intermediate',
        max_chunks: int = 5
    ) -> Dict[str, any]:
        """
        G√©n√®re une r√©ponse p√©dagogique adapt√©e au niveau
        
        Args:
            question: Question de l'√©tudiant
            context_chunks: Chunks r√©cup√©r√©s
            learning_level: Niveau (beginner/intermediate/advanced)
            max_chunks: Nombre max de chunks
            
        Returns:
            Dict avec answer, sources, question_type, suggestions
        """
        logger.info(f"üìö G√©n√©ration p√©dagogique (niveau: {learning_level})")
        
        if not context_chunks:
            return self._handle_no_context(question)
        
        # D√©tecter le type de question
        question_type = learning_config.detect_question_type(question)
        logger.info(f"Type de question d√©tect√© : {question_type}")
        
        # Limiter et formater le contexte
        limited_chunks = context_chunks[:max_chunks]
        context = self._format_educational_context(limited_chunks)
        
        # Construire le prompt p√©dagogique
        prompt = self._build_pedagogical_prompt(
            question, 
            context, 
            learning_level,
            question_type
        )
        
        # G√©n√©rer la r√©ponse
        if self.use_openai and hasattr(self, 'client'):
            answer = self._generate_with_openai_pedagogical(prompt, learning_level)
        else:
            answer = self._generate_with_local(prompt)
        
        # Ajouter des suggestions de suivi
        suggestions = self._get_follow_up_suggestions(question_type)
        
        return {
            'answer': answer,
            'sources': self._format_sources(limited_chunks),
            'context_used': len(limited_chunks),
            'question_type': question_type,
            'learning_level': learning_level,
            'follow_up_suggestions': suggestions
        }
    
    def _format_educational_context(self, chunks: List[Dict]) -> str:
        """Formate le contexte de mani√®re p√©dagogique"""
        context_parts = []
        
        for i, chunk in enumerate(chunks, 1):
            # Extraire les m√©tadonn√©es p√©dagogiques si disponibles
            doc_name = chunk.get('document_name', 'Document inconnu')
            content = chunk.get('content', '')
            
            # Formater avec num√©rotation claire
            context_parts.append(
                f"[Source {i} - {doc_name}]\n{content}\n"
            )
        
        return "\n".join(context_parts)
    
    def _build_pedagogical_prompt(
        self,
        question: str,
        context: str,
        level: str,
        question_type: str
    ) -> str:
        """Construit un prompt p√©dagogique adapt√©"""
        
        # Obtenir le template du niveau
        base_prompt = learning_config.LEARNING_PROMPTS.get(
            level, 
            learning_config.LEARNING_PROMPTS['intermediate']
        )
        
        # Ajouter des instructions sp√©cifiques selon le type
        type_instructions = {
            'definition': "\n- Commence par une d√©finition claire\n- Donne les caract√©ristiques principales\n- Explique l'importance",
            'explanation': "\n- Explique le principe de base\n- D√©cris le fonctionnement\n- Donne les raisons sous-jacentes",
            'example': "\n- Fournis 2-3 exemples concrets\n- Illustre avec des cas pratiques\n- Montre l'application",
            'comparison': "\n- Compare les concepts point par point\n- Souligne similitudes et diff√©rences\n- Indique quand utiliser chacun",
            'procedure': "\n- Pr√©sente les √©tapes de mani√®re ordonn√©e\n- Explique chaque √©tape\n- Donne des conseils pratiques"
        }
        
        specific_instructions = type_instructions.get(question_type, "")
        
        # Assembler le prompt complet
        full_prompt = base_prompt.format(
            context=context,
            question=question
        ) + specific_instructions
        
        return full_prompt
    
    def _generate_with_openai_pedagogical(self, prompt: str, level: str) -> str:
        """G√©n√®re avec OpenAI en mode p√©dagogique"""
        try:
            system_messages = {
                'beginner': "Tu es un professeur patient qui explique simplement aux d√©butants.",
                'intermediate': "Tu es un professeur qui guide les √©tudiants vers une compr√©hension approfondie.",
                'advanced': "Tu es un expert acad√©mique qui partage des connaissances avanc√©es."
            }
            
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_messages.get(level, system_messages['intermediate'])},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=400,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Erreur OpenAI : {e}")
            return self._create_extractive_answer_educational(prompt)
    
    def _generate_with_local(self, prompt: str) -> str:
        """G√©n√®re avec mod√®le local"""
        try:
            result = self.generator(prompt, max_length=500)[0]['generated_text']
            # Extraire seulement la r√©ponse g√©n√©r√©e
            answer = result.replace(prompt, "").strip()
            
            if len(answer) < 30:
                return self._create_extractive_answer_educational(prompt)
            
            return answer
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration : {e}")
            return self._create_extractive_answer_educational(prompt)
    
    def _create_extractive_answer_educational(self, context: str) -> str:
        """Cr√©e une r√©ponse extractive √©ducative"""
        # Extraire les phrases les plus pertinentes
        lines = context.split('\n')
        relevant_lines = [l for l in lines if l.strip() and not l.startswith('[')]
        
        if relevant_lines:
            return f"D'apr√®s le cours :\n\n" + "\n".join(relevant_lines[:5])
        return "Les informations demand√©es ne sont pas disponibles dans le cours."
    
    def _format_sources(self, chunks: List[Dict]) -> List[Dict]:
        """Formate les sources de mani√®re √©ducative"""
        return [
            {
                'document': chunk.get('document_name', 'Inconnu'),
                'chunk_id': chunk.get('chunk_id', ''),
                'score': chunk.get('score', 0),
                'page': chunk.get('page_number', 'N/A'),
                'topic': chunk.get('topic', 'G√©n√©ral')
            }
            for chunk in chunks
        ]
    
    def _get_follow_up_suggestions(self, question_type: str) -> List[str]:
        """Obtient des suggestions de questions de suivi"""
        suggestions = learning_config.FOLLOW_UP_SUGGESTIONS.get(
            question_type,
            [
                "Voulez-vous plus de d√©tails ?",
                "Souhaitez-vous voir un exemple ?",
                "Puis-je clarifier un point ?"
            ]
        )
        return suggestions[:3]  # Limiter √† 3 suggestions
    
    def _handle_no_context(self, question: str) -> Dict:
        """G√®re le cas o√π aucun contexte n'est trouv√©"""
        return {
            'answer': (
                "Je n'ai pas trouv√© d'information pertinente dans le cours pour r√©pondre √† cette question. "
                "Essayez de reformuler votre question ou v√©rifiez que le sujet est couvert dans les documents fournis."
            ),
            'sources': [],
            'context_used': 0,
            'question_type': 'general',
            'learning_level': 'intermediate',
            'follow_up_suggestions': [
                "Reformulez votre question",
                "V√©rifiez l'orthographe",
                "Pr√©cisez le sujet"
            ]
        }