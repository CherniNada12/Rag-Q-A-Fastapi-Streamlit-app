# üîß Guide d'Int√©gration - Learning Assistant RAG

Ce guide explique comment int√©grer et personnaliser le Learning Assistant RAG dans votre environnement.

## üìã Table des mati√®res
1. [Installation rapide](#installation-rapide)
2. [Configuration](#configuration)
3. [Int√©gration dans l'API existante](#int√©gration-api)
4. [Personnalisation des prompts](#personnalisation-prompts)
5. [Ajout de nouveaux types de questions](#nouveaux-types)
6. [Utilisation programmatique](#utilisation-programmatique)
7. [D√©ploiement](#d√©ploiement)

---

## üöÄ Installation rapide

### Linux/Mac
```bash
chmod +x start.sh
./start.sh
```

### Windows
```batch
start.bat
```

---

## ‚öôÔ∏è Configuration

### 1. Fichier `.env`

Cr√©ez un fichier `.env` √† la racine :

```properties
# === Mod√®les ===
EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL="gpt2"  # ou "openai" pour GPT

# === Configuration RAG ===
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K_RESULTS=5

# === API OpenAI (optionnel) ===
OPENAI_API_KEY="sk-..."

# === Serveurs ===
API_HOST="0.0.0.0"
API_PORT=8000
```

### 2. Mod√®les support√©s

#### Embeddings
- `sentence-transformers/all-MiniLM-L6-v2` (l√©ger, rapide)
- `sentence-transformers/all-mpnet-base-v2` (meilleur qualit√©)
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (multilingue)

#### LLM
- `gpt2` (local, gratuit)
- `gpt2-medium` (meilleur)
- `openai` avec `OPENAI_API_KEY` (meilleur qualit√©)

---

## üîå Int√©gration dans l'API existante

### M√©thode 1: Remplacer le g√©n√©rateur standard

Dans `src/api/main.py`, remplacez :

```python
from modules.generation import ResponseGenerator

generator = ResponseGenerator()
```

Par :

```python
from modules.learning_generator import LearningResponseGenerator

generator = LearningResponseGenerator(use_openai=False)  # ou True
```

### M√©thode 2: Ajouter un endpoint sp√©cial

Ajoutez un endpoint d√©di√© au Learning Assistant :

```python
from modules.learning_generator import LearningResponseGenerator

learning_generator = LearningResponseGenerator()

@app.post("/query/learning")
def query_learning(request: QueryRequest, level: str = 'intermediate'):
    """Endpoint p√©dagogique avec adaptation au niveau"""
    
    # Recherche
    chunks = retriever.search(request.question, top_k=request.top_k)
    
    # G√©n√©ration p√©dagogique
    result = learning_generator.generate_pedagogical_answer(
        question=request.question,
        context_chunks=chunks,
        learning_level=level
    )
    
    return {
        'answer': result['answer'],
        'sources': result['sources'],
        'question_type': result['question_type'],
        'level': result['learning_level'],
        'suggestions': result['follow_up_suggestions']
    }
```

---

## üìù Personnalisation des prompts

### 1. Modifier les prompts de niveau

√âditez `src/modules/learning_config.py` :

```python
LEARNING_PROMPTS = {
    'beginner': """
    Tu es un tuteur bienveillant pour d√©butants.
    
    R√®gles :
    - Vocabulaire simple
    - Analogies concr√®tes
    - Exemples du quotidien
    - Encouragements
    
    Contexte: {context}
    Question: {question}
    
    R√©ponds simplement :
    """,
    
    'intermediate': """
    Tu es un professeur exp√©riment√©.
    
    R√®gles :
    - Explications structur√©es
    - Liens entre concepts
    - Exemples vari√©s
    - Pistes de r√©flexion
    
    Contexte: {context}
    Question: {question}
    
    R√©ponds de mani√®re claire :
    """,
    
    'advanced': """
    Tu es un expert acad√©mique.
    
    R√®gles :
    - Rigueur technique
    - Citations pr√©cises
    - Approfondissements
    - Nuances et subtilit√©s
    
    Contexte: {context}
    Question: {question}
    
    R√©ponds avec pr√©cision :
    """
}
```

### 2. Personnaliser les formats de r√©ponse

Modifiez `RESPONSE_FORMATS` :

```python
RESPONSE_FORMATS = {
    'definition': {
        'intro': 'Voici une d√©finition claire :',
        'structure': [
            '1. D√©finition de base',
            '2. Caract√©ristiques principales',
            '3. Importance dans le domaine'
        ]
    },
    'your_custom_type': {
        'intro': 'Votre introduction personnalis√©e',
        'structure': ['Point 1', 'Point 2', 'Point 3']
    }
}
```

---

## ‚ûï Ajout de nouveaux types de questions

### 1. D√©finir les mots-cl√©s

Dans `learning_config.py` :

```python
QUESTION_TYPES = {
    # Types existants...
    'definition': ['c\'est quoi', 'd√©finir'],
    'explanation': ['comment', 'pourquoi'],
    
    # Nouveau type
    'exercise': ['exercice', 'pratique', 'entra√Ænement', 's\'exercer'],
    'quiz': ['quiz', 'test', '√©valuation', 'qcm']
}
```

### 2. Cr√©er le format de r√©ponse

```python
RESPONSE_FORMATS = {
    # Formats existants...
    
    'exercise': {
        'intro': 'Voici un exercice pratique :',
        'structure': [
            '√ânonc√©',
            'Consignes',
            'Indice',
            'Correction guid√©e'
        ]
    },
    'quiz': {
        'intro': 'Voici un quiz pour tester vos connaissances :',
        'structure': [
            'Questions',
            'Options de r√©ponse',
            'Explications'
        ]
    }
}
```

### 3. Ajouter les suggestions de suivi

```python
FOLLOW_UP_SUGGESTIONS = {
    # Suggestions existantes...
    
    'exercise': [
        "Voulez-vous la correction d√©taill√©e ?",
        "Souhaitez-vous un exercice similaire ?",
        "Dois-je expliquer la th√©orie derri√®re ?"
    ],
    'quiz': [
        "Voulez-vous plus de questions ?",
        "Souhaitez-vous approfondir un sujet ?",
        "Dois-je expliquer les erreurs communes ?"
    ]
}
```

---

## üíª Utilisation programmatique

### Exemple simple

```python
from modules.retrieval import FAISSRetriever
from modules.learning_generator import LearningResponseGenerator

# Initialiser
retriever = FAISSRetriever()
generator = LearningResponseGenerator()

# Charger l'index existant
retriever.load_index()

# Poser une question
question = "C'est quoi le deep learning ?"
chunks = retriever.search(question, top_k=5)

# G√©n√©rer une r√©ponse pour d√©butant
result = generator.generate_pedagogical_answer(
    question=question,
    context_chunks=chunks,
    learning_level='beginner'
)

print(result['answer'])
```

### Exemple avanc√© avec traitement de batch

```python
questions = [
    ("C'est quoi l'IA ?", 'beginner'),
    ("Comment fonctionne le backprop ?", 'intermediate'),
    ("Optimise un r√©seau de neurones", 'advanced')
]

results = []
for question, level in questions:
    chunks = retriever.search(question, top_k=5)
    result = generator.generate_pedagogical_answer(
        question=question,
        context_chunks=chunks,
        learning_level=level
    )
    results.append(result)

# Exporter
import json
with open('batch_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

---

## üöÄ D√©ploiement

### Docker

Cr√©ez un `Dockerfile` :

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build et run :

```bash
docker build -t learning-assistant-rag .
docker run -p 8000:8000 -v $(pwd)/data:/app/data learning-assistant-rag
```

### Docker Compose

`docker-compose.yml` :

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
    environment:
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - LLM_MODEL=gpt2
    
  streamlit:
    build: .
    command: streamlit run src/frontend/learning_app.py
    ports:
      - "8501:8501"
    depends_on:
      - api
```

Lancer :

```bash
docker-compose up -d
```

### Production (Nginx + Gunicorn)

1. Installer Gunicorn :
```bash
pip install gunicorn
```

2. Lancer avec Gunicorn :
```bash
gunicorn src.api.main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
```

3. Configuration Nginx :
```nginx
server {
    listen 80;
    server_name your-domain.com;

    location /api/ {
        proxy_pass http://localhost:8000/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    location / {
        proxy_pass http://localhost:8501;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

---

## üß™ Tests d'int√©gration

```python
import pytest
from modules.learning_generator import LearningResponseGenerator

def test_beginner_response():
    generator = LearningResponseGenerator()
    
    chunks = [{'content': 'Test content', 'document_name': 'test.txt'}]
    result = generator.generate_pedagogical_answer(
        "C'est quoi le ML ?",
        chunks,
        'beginner'
    )
    
    assert result['learning_level'] == 'beginner'
    assert len(result['answer']) > 0
    assert 'follow_up_suggestions' in result

def test_question_type_detection():
    from modules.learning_config import learning_config
    
    assert learning_config.detect_question_type("C'est quoi ?") == 'definition'
    assert learning_config.detect_question_type("Comment √ßa marche ?") == 'explanation'
```

---

## üìö Ressources suppl√©mentaires

- [Documentation Sentence Transformers](https://www.sbert.net/)
- [FAISS Documentation](https://github.com/facebookresearch/faiss/wiki)
- [FastAPI Guide](https://fastapi.tiangolo.com/)
- [Streamlit Docs](https://docs.streamlit.io/)

---

## ‚ùì FAQ

**Q: Comment changer de mod√®le d'embedding ?**
R: Modifiez `EMBEDDING_MODEL` dans `.env` et relancez le syst√®me.

**Q: Puis-je utiliser GPT-4 ?**
R: Oui, mettez votre cl√© OpenAI et passez `use_openai=True` au g√©n√©rateur.

**Q: Comment ajouter le support multilingue ?**
R: Utilisez `paraphrase-multilingual-MiniLM-L12-v2` comme mod√®le d'embedding.

**Q: L'index persiste-t-il entre les red√©marrages ?**
R: Oui, l'index est sauvegard√© dans `data/index/`.

---

Pour plus d'aide, ouvrez une issue sur GitHub ou consultez la documentation compl√®te.