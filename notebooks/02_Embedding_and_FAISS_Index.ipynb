{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 2: Embedding and FAISS Index\n",
    "# Ce notebook montre comment cr√©er des embeddings et un index FAISS\n",
    "\n",
    "# Cellule 1: Imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from modules.embeddings import EmbeddingModel\n",
    "from modules.retrieval import FAISSRetriever\n",
    "from modules.config import config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"‚úÖ Modules import√©s\")\n",
    "\n",
    "# Cellule 2: Initialiser le mod√®le d'embedding\n",
    "embedding_model = EmbeddingModel()\n",
    "print(f\"Mod√®le: {embedding_model.model_name}\")\n",
    "print(f\"Dimension: {embedding_model.get_embedding_dimension()}\")\n",
    "\n",
    "# Cellule 3: Tester avec des exemples simples\n",
    "sample_texts = [\n",
    "    \"L'intelligence artificielle transforme notre monde\",\n",
    "    \"Le machine learning permet aux machines d'apprendre\",\n",
    "    \"Les r√©seaux de neurones sont inspir√©s du cerveau humain\",\n",
    "    \"Python est un langage de programmation populaire\",\n",
    "    \"La cuisine fran√ßaise est r√©put√©e dans le monde entier\"\n",
    "]\n",
    "\n",
    "# G√©n√©rer les embeddings\n",
    "embeddings = embedding_model.encode(sample_texts)\n",
    "print(f\"\\nüìä Shape des embeddings: {embeddings.shape}\")\n",
    "print(f\"Type: {embeddings.dtype}\")\n",
    "\n",
    "# Cellule 4: Visualiser les embeddings (r√©duction en 2D avec PCA)\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100)\n",
    "\n",
    "for i, txt in enumerate(sample_texts):\n",
    "    plt.annotate(\n",
    "        txt[:30] + \"...\", \n",
    "        (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.title(\"Visualisation des embeddings (PCA 2D)\")\n",
    "plt.xlabel(\"Composante 1\")\n",
    "plt.ylabel(\"Composante 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cellule 5: Calculer les similarit√©s\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(embeddings)\n",
    "print(\"\\nüîç Matrice de similarit√© cosinus:\")\n",
    "print(similarities)\n",
    "\n",
    "# Trouver les paires les plus similaires\n",
    "for i in range(len(sample_texts)):\n",
    "    for j in range(i+1, len(sample_texts)):\n",
    "        sim = similarities[i][j]\n",
    "        print(f\"\\nSimilarit√© entre:\")\n",
    "        print(f\"  '{sample_texts[i][:40]}'\")\n",
    "        print(f\"  '{sample_texts[j][:40]}'\")\n",
    "        print(f\"  Score: {sim:.4f}\")\n",
    "\n",
    "# Cellule 6: Cr√©er un index FAISS\n",
    "retriever = FAISSRetriever()\n",
    "\n",
    "# Cr√©er l'index avec nos exemples\n",
    "metadata = [\n",
    "    {'chunk_id': f'chunk_{i}', 'content': txt, 'document_name': 'example.txt'}\n",
    "    for i, txt in enumerate(sample_texts)\n",
    "]\n",
    "\n",
    "retriever.create_index(embeddings, metadata)\n",
    "print(f\"\\n‚úÖ Index FAISS cr√©√© avec {retriever.index.ntotal} vecteurs\")\n",
    "\n",
    "# Cellule 7: Tester la recherche\n",
    "query = \"Comment fonctionnent les r√©seaux de neurones ?\"\n",
    "print(f\"\\nüîç Question: {query}\")\n",
    "\n",
    "results = retriever.search(query, top_k=3)\n",
    "\n",
    "print(\"\\nüìã R√©sultats:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Score: {result['score']:.4f}\")\n",
    "    print(f\"   Contenu: {result['content']}\")\n",
    "    print(f\"   Chunk ID: {result['chunk_id']}\")\n",
    "\n",
    "# Cellule 8: Charger des chunks r√©els (du notebook pr√©c√©dent)\n",
    "import json\n",
    "\n",
    "chunks_file = config.CHUNKS_DIR / \"votre_document.pdf_chunks.json\"\n",
    "\n",
    "if chunks_file.exists():\n",
    "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìÑ Chunks charg√©s: {len(chunks)}\")\n",
    "    \n",
    "    # G√©n√©rer les embeddings\n",
    "    chunk_texts = [chunk['content'] for chunk in chunks]\n",
    "    chunk_embeddings = embedding_model.encode(chunk_texts)\n",
    "    \n",
    "    # Cr√©er l'index\n",
    "    retriever_full = FAISSRetriever()\n",
    "    retriever_full.create_index(chunk_embeddings, chunks)\n",
    "    \n",
    "    print(f\"‚úÖ Index cr√©√© avec {retriever_full.index.ntotal} vecteurs\")\n",
    "    \n",
    "    # Sauvegarder\n",
    "    retriever_full.save_index()\n",
    "    print(f\"‚úÖ Index sauvegard√© dans {config.INDEX_DIR}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun fichier de chunks trouv√©. Ex√©cutez d'abord le notebook 01.\")\n",
    "\n",
    "# Cellule 9: Test de recherche sur l'index complet\n",
    "if 'retriever_full' in locals():\n",
    "    test_query = \"Quel est le sujet principal du document ?\"\n",
    "    print(f\"\\nüîç Question: {test_query}\")\n",
    "    \n",
    "    results = retriever_full.search(test_query, top_k=5)\n",
    "    \n",
    "    print(\"\\nüìã Top 5 r√©sultats:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {result['document_name']} - Chunk {result['chunk_index']}\")\n",
    "        print(f\"   Score: {result['score']:.4f}\")\n",
    "        print(f\"   Extrait: {result['content'][:150]}...\")\n",
    "\n",
    "# Cellule 10: Statistiques de l'index\n",
    "if 'retriever_full' in locals():\n",
    "    print(\"\\nüìä Statistiques de l'index:\")\n",
    "    print(f\"Nombre total de vecteurs: {retriever_full.index.ntotal}\")\n",
    "    print(f\"Dimension des vecteurs: {retriever_full.dimension}\")\n",
    "    print(f\"Nombre de documents uniques: {len(set(c['document_name'] for c in retriever_full.metadata))}\")\n",
    "    \n",
    "    # Distribution des chunks par document\n",
    "    from collections import Counter\n",
    "    doc_counts = Counter(c['document_name'] for c in retriever_full.metadata)\n",
    "    \n",
    "    print(\"\\nüìö R√©partition des chunks par document:\")\n",
    "    for doc, count in doc_counts.items():\n",
    "        print(f\"  {doc}: {count} chunks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71b9867420a54b8128b13ab02ac6735a219956841e8280f4043e92289482a8dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
