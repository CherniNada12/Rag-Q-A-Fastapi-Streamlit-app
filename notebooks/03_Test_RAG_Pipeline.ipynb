{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 3: Test RAG Pipeline\n",
    "# Ce notebook teste le pipeline complet RAG\n",
    "\n",
    "# Cellule 1: Imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from modules.ingestion import DocumentIngestion\n",
    "from modules.chunking import TextChunker\n",
    "from modules.embeddings import EmbeddingModel\n",
    "from modules.retrieval import FAISSRetriever\n",
    "from modules.generation import ResponseGenerator\n",
    "from modules.config import config\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ Tous les modules import√©s\")\n",
    "\n",
    "# Cellule 2: Initialiser tous les composants\n",
    "print(\"üöÄ Initialisation du pipeline RAG...\")\n",
    "\n",
    "ingestion = DocumentIngestion()\n",
    "chunker = TextChunker()\n",
    "retriever = FAISSRetriever()\n",
    "generator = ResponseGenerator()\n",
    "\n",
    "print(\"‚úÖ Pipeline initialis√©\")\n",
    "\n",
    "# Cellule 3: Fonction helper pour le pipeline complet\n",
    "def process_document_full_pipeline(file_path: Path):\n",
    "    \"\"\"\n",
    "    Pipeline complet : ingestion ‚Üí chunking ‚Üí embedding ‚Üí indexation\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìÑ Traitement de: {file_path.name}\")\n",
    "    \n",
    "    # 1. Ingestion\n",
    "    print(\"1Ô∏è‚É£ Extraction du texte...\")\n",
    "    doc_info = ingestion.process_document(file_path)\n",
    "    print(f\"   ‚úÖ {doc_info['num_characters']} caract√®res extraits\")\n",
    "    \n",
    "    # 2. Chunking\n",
    "    print(\"2Ô∏è‚É£ Cr√©ation des chunks...\")\n",
    "    chunks = chunker.create_chunks_with_metadata(\n",
    "        doc_info['content'],\n",
    "        doc_info['filename']\n",
    "    )\n",
    "    print(f\"   ‚úÖ {len(chunks)} chunks cr√©√©s\")\n",
    "    \n",
    "    # 3. Embeddings\n",
    "    print(\"3Ô∏è‚É£ G√©n√©ration des embeddings...\")\n",
    "    chunk_texts = [c['content'] for c in chunks]\n",
    "    embeddings = retriever.embedding_model.encode(chunk_texts)\n",
    "    print(f\"   ‚úÖ Embeddings: {embeddings.shape}\")\n",
    "    \n",
    "    # 4. Indexation\n",
    "    print(\"4Ô∏è‚É£ Ajout √† l'index FAISS...\")\n",
    "    retriever.add_to_index(embeddings, chunks)\n",
    "    print(f\"   ‚úÖ Index: {retriever.index.ntotal} vecteurs\")\n",
    "    \n",
    "    return {\n",
    "        'doc_info': doc_info,\n",
    "        'chunks': chunks,\n",
    "        'num_vectors': retriever.index.ntotal\n",
    "    }\n",
    "\n",
    "# Cellule 4: Traiter un ou plusieurs documents\n",
    "documents_dir = config.DOCUMENTS_DIR\n",
    "pdf_files = list(documents_dir.glob(\"*.pdf\"))\n",
    "txt_files = list(documents_dir.glob(\"*.txt\"))\n",
    "\n",
    "all_files = pdf_files + txt_files\n",
    "\n",
    "if all_files:\n",
    "    print(f\"\\nüìö {len(all_files)} document(s) trouv√©(s)\")\n",
    "    \n",
    "    for file_path in all_files[:3]:  # Limiter √† 3 pour le test\n",
    "        try:\n",
    "            result = process_document_full_pipeline(file_path)\n",
    "            print(f\"‚úÖ {file_path.name} trait√© avec succ√®s\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur avec {file_path.name}: {e}\")\n",
    "    \n",
    "    # Sauvegarder l'index\n",
    "    retriever.save_index()\n",
    "    print(f\"\\nüíæ Index sauvegard√©\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun document trouv√© dans data/documents/\")\n",
    "\n",
    "# Cellule 5: Fonction pour tester une question\n",
    "def ask_question(question: str, top_k: int = 5, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Pose une question au syst√®me RAG\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n‚ùì Question: {question}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Recherche\n",
    "    if verbose:\n",
    "        print(\"\\nüîç Recherche des passages pertinents...\")\n",
    "    retrieved_chunks = retriever.search(question, top_k=top_k)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úÖ {len(retrieved_chunks)} chunks r√©cup√©r√©s\")\n",
    "        print(\"\\nüìÑ Top 3 passages:\")\n",
    "        for i, chunk in enumerate(retrieved_chunks[:3], 1):\n",
    "            print(f\"\\n{i}. Score: {chunk['score']:.4f}\")\n",
    "            print(f\"   Document: {chunk['document_name']}\")\n",
    "            print(f\"   Extrait: {chunk['content'][:200]}...\")\n",
    "    \n",
    "    # 2. G√©n√©ration\n",
    "    if verbose:\n",
    "        print(\"\\nü§ñ G√©n√©ration de la r√©ponse...\")\n",
    "    response = generator.generate_answer(question, retrieved_chunks)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nüí° R√âPONSE:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(response['answer'])\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nüìö Sources: {response['context_used']} chunks utilis√©s\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Cellule 6: Tests avec diff√©rentes questions\n",
    "if retriever.index and retriever.index.ntotal > 0:\n",
    "    questions = [\n",
    "        \"Quel est le sujet principal du document ?\",\n",
    "        \"Quels sont les points cl√©s abord√©s ?\",\n",
    "        \"Y a-t-il des dates importantes mentionn√©es ?\",\n",
    "        \"Qui sont les personnes cit√©es ?\",\n",
    "        \"Quelles sont les conclusions ?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üß™ TEST DU PIPELINE RAG\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for question in questions[:2]:  # Tester 2 questions\n",
    "        response = ask_question(question)\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun document index√©. Ex√©cutez d'abord la cellule 4.\")\n",
    "\n",
    "# Cellule 7: √âvaluation de la pertinence\n",
    "def evaluate_retrieval(question: str, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    √âvalue la qualit√© de la recherche\n",
    "    \"\"\"\n",
    "    results = retriever.search(question, top_k=top_k)\n",
    "    \n",
    "    scores = [r['score'] for r in results]\n",
    "    \n",
    "    print(f\"\\nüìä Analyse pour: '{question}'\")\n",
    "    print(f\"Nombre de r√©sultats: {len(results)}\")\n",
    "    print(f\"Score moyen: {sum(scores)/len(scores):.4f}\")\n",
    "    print(f\"Score max: {max(scores):.4f}\")\n",
    "    print(f\"Score min: {min(scores):.4f}\")\n",
    "    \n",
    "    # Afficher les scores\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(scores)), scores)\n",
    "    plt.xlabel(\"Rang du r√©sultat\")\n",
    "    plt.ylabel(\"Score de similarit√©\")\n",
    "    plt.title(f\"Distribution des scores de pertinence\\n'{question}'\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Cellule 8: Test d'√©valuation\n",
    "if retriever.index and retriever.index.ntotal > 0:\n",
    "    test_query = \"Quelles sont les informations principales ?\"\n",
    "    eval_results = evaluate_retrieval(test_query)\n",
    "\n",
    "# Cellule 9: Comparaison de diff√©rentes questions\n",
    "def compare_queries(queries: list):\n",
    "    \"\"\"\n",
    "    Compare les performances pour diff√©rentes questions\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    comparison = []\n",
    "    \n",
    "    for query in queries:\n",
    "        results = retriever.search(query, top_k=5)\n",
    "        avg_score = sum(r['score'] for r in results) / len(results)\n",
    "        max_score = max(r['score'] for r in results)\n",
    "        \n",
    "        comparison.append({\n",
    "            'Question': query[:50] + \"...\",\n",
    "            'Score moyen': f\"{avg_score:.4f}\",\n",
    "            'Score max': f\"{max_score:.4f}\",\n",
    "            'Nb r√©sultats': len(results)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison)\n",
    "    return df\n",
    "\n",
    "# Test de comparaison\n",
    "if retriever.index and retriever.index.ntotal > 0:\n",
    "    test_queries = [\n",
    "        \"Quel est le contexte ?\",\n",
    "        \"Quels sont les objectifs ?\",\n",
    "        \"Quelles sont les m√©thodes utilis√©es ?\",\n",
    "        \"Quels sont les r√©sultats obtenus ?\"\n",
    "    ]\n",
    "    \n",
    "    comparison_df = compare_queries(test_queries)\n",
    "    print(\"\\nüìä Comparaison des requ√™tes:\")\n",
    "    display(comparison_df)\n",
    "\n",
    "# Cellule 10: Export des r√©sultats\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_test_results(question: str, response: dict, output_dir: Path = None):\n",
    "    \"\"\"\n",
    "    Exporte les r√©sultats d'un test\n",
    "    \"\"\"\n",
    "    output_dir = output_dir or config.DATA_DIR / \"test_results\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = output_dir / f\"test_{timestamp}.json\"\n",
    "    \n",
    "    test_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'question': question,\n",
    "        'answer': response['answer'],\n",
    "        'sources': response['sources'],\n",
    "        'context_used': response['context_used']\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ R√©sultats export√©s: {output_file}\")\n",
    "\n",
    "# Exemple d'export\n",
    "if 'response' in locals():\n",
    "    export_test_results(questions[0], response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71b9867420a54b8128b13ab02ac6735a219956841e8280f4043e92289482a8dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
