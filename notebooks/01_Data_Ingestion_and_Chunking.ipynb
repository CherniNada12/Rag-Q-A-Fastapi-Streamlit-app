{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 1: Data Ingestion and Chunking\n",
    "# Ce notebook montre comment extraire et d√©couper des documents\n",
    "import docx\n",
    "# Cellule 1: Imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from modules.ingestion import DocumentIngestion\n",
    "from modules.chunking import TextChunker\n",
    "from modules.config import config\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚úÖ Modules import√©s avec succ√®s\")\n",
    "\n",
    "# Cellule 2: Initialisation\n",
    "ingestion = DocumentIngestion()\n",
    "chunker = TextChunker(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "print(f\"Chunk size: {chunker.chunk_size}\")\n",
    "print(f\"Chunk overlap: {chunker.chunk_overlap}\")\n",
    "\n",
    "# Cellule 3: Exemple avec un fichier texte simple\n",
    "sample_text = \"\"\"\n",
    "L'intelligence artificielle (IA) est un ensemble de th√©ories et de techniques \n",
    "mises en ≈ìuvre en vue de r√©aliser des machines capables de simuler l'intelligence humaine.\n",
    "\n",
    "Le machine learning est une branche de l'IA qui permet aux ordinateurs d'apprendre \n",
    "√† partir de donn√©es sans √™tre explicitement programm√©s.\n",
    "\n",
    "Le deep learning est une sous-cat√©gorie du machine learning qui utilise des \n",
    "r√©seaux de neurones artificiels avec plusieurs couches cach√©es.\n",
    "\"\"\"\n",
    "\n",
    "# Tester le chunking\n",
    "chunks = chunker.split_into_chunks(sample_text)\n",
    "print(f\"\\nüìä Nombre de chunks cr√©√©s: {len(chunks)}\")\n",
    "print(\"\\nüìÑ Premier chunk:\")\n",
    "print(chunks[0])\n",
    "\n",
    "# Cellule 4: Cr√©er des chunks avec m√©tadonn√©es\n",
    "chunks_with_meta = chunker.create_chunks_with_metadata(\n",
    "    sample_text, \n",
    "    \"exemple.txt\"\n",
    ")\n",
    "\n",
    "# Afficher sous forme de DataFrame\n",
    "df_chunks = pd.DataFrame(chunks_with_meta)\n",
    "print(\"\\nüìä Chunks avec m√©tadonn√©es:\")\n",
    "display(df_chunks[['chunk_id', 'chunk_index', 'num_words', 'num_characters']])\n",
    "\n",
    "# Cellule 5: Traiter un vrai document\n",
    "# Assurez-vous d'avoir un fichier PDF dans data/documents/\n",
    "doc_path = config.DOCUMENTS_DIR / \"votre_document.pdf\"  # Remplacer par un vrai fichier\n",
    "\n",
    "if doc_path.exists():\n",
    "    print(f\"üìÑ Traitement de: {doc_path.name}\")\n",
    "    \n",
    "    # Extraction\n",
    "    doc_info = ingestion.process_document(doc_path)\n",
    "    print(f\"‚úÖ Texte extrait: {doc_info['num_characters']} caract√®res\")\n",
    "    \n",
    "    # Chunking\n",
    "    chunks = chunker.create_chunks_with_metadata(\n",
    "        doc_info['content'],\n",
    "        doc_info['filename']\n",
    "    )\n",
    "    print(f\"‚úÖ Chunks cr√©√©s: {len(chunks)}\")\n",
    "    \n",
    "    # Visualiser\n",
    "    df = pd.DataFrame(chunks)\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Veuillez placer un document PDF dans data/documents/\")\n",
    "\n",
    "# Cellule 6: Statistiques sur les chunks\n",
    "if 'chunks' in locals():\n",
    "    stats = {\n",
    "        'total_chunks': len(chunks),\n",
    "        'avg_words': sum(c['num_words'] for c in chunks) / len(chunks),\n",
    "        'avg_chars': sum(c['num_characters'] for c in chunks) / len(chunks),\n",
    "        'min_words': min(c['num_words'] for c in chunks),\n",
    "        'max_words': max(c['num_words'] for c in chunks)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä Statistiques des chunks:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "# Cellule 7: Sauvegarder les chunks (optionnel)\n",
    "import json\n",
    "\n",
    "if 'chunks' in locals():\n",
    "    output_path = config.CHUNKS_DIR / f\"{doc_info['filename']}_chunks.json\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Chunks sauvegard√©s: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71b9867420a54b8128b13ab02ac6735a219956841e8280f4043e92289482a8dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
